# Fundamentals of CUDA

<img src="../images/gpu-architecture.png" alt="CUDA Architecture" width="400">

## CPU vs GPU Architecture: Parallel Processing Explained
This image compares a CPU (left) and a GPU (right), highlighting key differences.

**CPU Architecture (Left):**
- Few powerful cores with dedicated L1, L2, and L3 caches.
- Optimized for sequential and general-purpose tasks.

**GPU Architecture (Right):**
- Thousands of smaller cores designed for parallel compute.
- Focus on L2 cache and DRAM for high-throughput workloads.

CUDA (Compute Unified Device Architecture) is a parallel computing platform and programming model developed by NVIDIA that enables developers to use GPUs for general-purpose computation. It provides direct access to the GPUâ€™s massive parallel processing capabilities.

<hr style="height: 1px; border: none; background-color: #333;">

**What is Thread?**  
A thread is the smallest unit of execution within a process. It represents a sequence of instructions that can be executed independently while sharing resources (such as memory) with other threads in the same process.

**Threads in CPU vs. GPU**

*CPU Threads:*
- CPUs have a small number of powerful cores, and each core can run multiple threads using techniques like hyperthreading.
- Suited for sequential tasks and workloads that require complex logic and branching.

*GPU Threads:*
- GPUs contain thousands of lightweight threads running in parallel, making them highly efficient for data-parallel computations (e.g., matrix operations in deep learning).
- Threads are organized into groups called warps (in CUDA) for massive parallelism.

<hr style="height: 1px; border: none; background-color: #333;">

**What is a Block?**
A **block** is a group of threads that execute the same kernel function on a GPU and can cooperate by sharing fast on-chip memory (shared memory).  
- Threads within a block can synchronize using functions like `__syncthreads()`.
- Each block has its own unique index (`blockIdx.x`, `blockIdx.y`, etc.) and its threads are indexed via `threadIdx.x`, `threadIdx.y`, etc.

**What is a Grid?**
A **grid** is a collection of blocks that together execute a kernel across the entire data set.  
- Blocks in a grid run independently and do not share memory directly.
- The grid is organized in dimensions (e.g., `gridDim.x`, `gridDim.y`) that define the total number of blocks executing the kernel.

<hr style="height: 1px; border: none; background-color: #333;">

**Hierarchy Recap:**
- **Grid:** Collection of blocks.
- **Block:** Group of threads that share memory and can synchronize.
- **Thread:** Smallest unit of execution.

This hierarchy allows CUDA to effectively distribute and manage parallel computations on GPUs.
